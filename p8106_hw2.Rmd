---
title: "p8106_hw2"
author: "Hao Zheng(hz2770)"
date: "2022/3/5"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(mgcv)
library(nlme)
library(gridExtra)
```

```{r, warning = FALSE}
# Data Cleaning
dat = 
  read.csv("./data/college.csv")[-1] %>% 
  janitor::clean_names() %>% 
  na.omit()

# Data Partition
indexTrain <- createDataPartition(y = dat$outstate, p = 0.8, list = FALSE)
trainData <- dat[indexTrain,]
testData <- dat[-indexTrain,]
head(trainData)
```


## Exploratory Data Analysis
```{r}
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$psh <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)

x <- trainData %>% select(-outstate)
y <- trainData$outstate

# scatter plot
featurePlot(x,
            y,
            plot = "scatter",
            span = .5,
            labels = c("Predictors", "outstate"),
            type = c("p", "smooth"),
            layout = c(4,4))
```

From the scatter plot, we can see that most predictors are not linearly associated with the response variable. However, there may exist a linear relationship between the variable `perc_alumni`, `grad_rate`, `room_board` and the response `outstate` respectively.

## Smoothing Spline Models
Now let's fit smoothing spline models using `terminal` as the only predictor of `outstate`.
```{r}
terminal.grid <- seq(from = 40, to = 100, by = 1)
fit.ss <- smooth.spline(trainData$terminal, trainData$outstate, cv = TRUE)
fit.ss$df
fit.ss$lambda

pred.ss <- predict(fit.ss,
                   x = terminal.grid)
pred.ss.df <- data.frame(pred = pred.ss$y,
                         terminal = terminal.grid)

# plot the fit
p <- ggplot(data = trainData, aes(x = terminal, y = outstate)) + 
  geom_point(color = rgb(.2, .4, .2, .5))

p + 
  geom_line(aes(x = terminal.grid, y = pred), data = pred.ss.df, color = rgb(.8, .1, .1, 1)) + theme_bw()
```

The optimal smoothing spline model fitted using the degrees of freedom obtained by generalized cross validation is as above with degrees of freedom `r fit.ss$df`. As we can see from the plot, the smoothing spline obtained is quite smooth and fits the data quite well.

Then we also try to fit the model for a range of degrees of freedom to observe the underlying pattern.

```{r}
# Write a function about it
ss.func <- function(degree){
  fit.ss <- smooth.spline(trainData$terminal, trainData$outstate, df = degree)
  pred.ss <- predict(fit.ss,
                     x = terminal.grid)
  pred.ss.df <- data.frame(pred = pred.ss$y,
                           terminal = terminal.grid,
                           df = degree)
}

ss.list <- list()
for (i in 2:15) {
  ss.list <- rbind(ss.list, ss.func(i))
}
ss.data <- as.data.frame(ss.list)

p +
  geom_line(aes(x = terminal, y = pred, group = df, color = df), data = ss.data)
```

From the plot of smoothing spline fit with different degrees of freedom, we can see that when the degrees of freedom is small, the fitted line is quite linear, and it gets more and more wiggly as degrees of freedom increase.

## Generalized Additive Models (GAM)
Fit GAM model with all the predictors.
```{r}
set.seed(2022)
ctrl = trainControl(method = "cv", number = 10)

model.gam <- train(x, y,
                 method = "gam",
                 tuneGrid = data.frame(method = "GCV.Cp",
                                       select = TRUE),
                 trControl = ctrl)
model.gam$finalModel
summary(model.gam)
```

According to the p value, some predictors may not be significant in the GAM model, such as `terminal`, `top25perc` and `p_undergrad`. The deviance explained by the model is 85.3%, adjusted R-squared value is 0.833, which is quite close to 1. The GAM model fits the data quite well.

The plots of each predictor against the response variable are as below.
```{r}
plot(model.gam$finalModel)
```

Now let's calculate the test error of the GAM model.
```{r}
test_x = testData %>% select(-outstate)

gam.pred <- predict(model.gam, newdata = test_x)

test_error_gam = mean((gam.pred - testData$outstate)^2)
test_error_gam
```

The test error for the GAM model is `r test_error_gam`.

## Multivariate Adaptive Regression Spline (MARS)
```{r}
set.seed(2022)

model.mars <- train(x,y,
                    method = "earth",
                    tuneGrid = expand.grid(degree = 1:3,
                                           nprune = 2:25),
                    trControl = ctrl)
plot(model.mars)

model.mars$bestTune
summary(model.mars$finalModel)
coef(model.mars$finalModel)
```

The final model uses 2 product degree and `nprune` = 16. 13 terms are selected of 33 terms among 9 of the 16 predictors. The predictors `expend`, `room_board`, `perc_alumni` and `accept` are of the most importance.

Then we calculate the test error on the test data.
```{r}
mars.pred <- predict(model.mars, newdata = test_x)

test_error_mars = mean((mars.pred - testData$outstate)^2)
test_error_mars
```

The test error is `r test_error_mars`.

```{r}
# Perform partial dependency plot.
p1 <- pdp::partial(model.mars, pred.var = c("grad_rate"), grid.resolution = 10) %>% autoplot()
p2 <- pdp::partial(model.mars, pred.var = c("grad_rate","accept"), grid.resolution = 10) %>% 
  pdp::plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, screen = list(z = 20, x = -60))

grid.arrange(p1, p2, ncol = 2)
```

We present two partial dependency plot here, the left one is for `grad_rate` while the right one is for both variables `grad_rate` and `accept`.

## Model Selection
```{r}
set.seed(2022)
model.lm <- train(x, y,
                  method = "lm",
                  trControl = ctrl)

resamp <- resamples(list(MARS = model.mars,
                         LM = model.lm))
summary(resamp)

bwplot(resamp, metric = "RMSE")
```

As we learned in the class, final model selection should be based on our cross-validation results. Since the MARS model has far less RMSE than the linear model, so we prefer the use of model MARS when predicting the out-of-state tuition.